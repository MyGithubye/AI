{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t09eeeR5prIJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# 循环神经网络（RNN）文本生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hcD2nPQvPOFM"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://tensorflow.google.cn/tutorials/text/text_generation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />在 tensorflow.google.cn 上查看</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/zh-cn/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />在 Google Colab 运行</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/zh-cn/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />在 GitHub 上查看源代码</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/zh-cn/tutorials/text/text_generation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />下载此 notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BwpJ5IffzRG6"
   },
   "source": [
    "本教程演示如何使用基于字符的 RNN 生成文本。我们将使用 Andrej Karpathy 在[《循环神经网络不合理的有效性》](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)一文中提供的莎士比亚作品数据集。给定此数据中的一个字符序列 （“Shakespear”），训练一个模型以预测该序列的下一个字符（“e”）。通过重复调用该模型，可以生成更长的文本序列。\n",
    "\n",
    "请注意：启用 GPU 加速可以更快地执行此笔记本。在 Colab 中依次选择：*运行时 > 更改运行时类型 > 硬件加速器 > GPU*。如果在本地运行，请确保 TensorFlow 的版本为 1.11 或更高。\n",
    "\n",
    "本教程包含使用 [tf.keras](https://www.tensorflow.org/programmers_guide/keras) 和 [eager execution](https://www.tensorflow.org/programmers_guide/eager) 实现的可运行代码。以下是当本教程中的模型训练 30 个周期 （epoch），并以字符串 “Q” 开头时的示例输出：\n",
    "\n",
    "<pre>\n",
    "QUEENE:\n",
    "I had thought thou hadst a Roman; for the oracle,\n",
    "Thus by All bids the man against the word,\n",
    "Which are so weak of care, by old care done;\n",
    "Your children were in your holy love,\n",
    "And the precipitation through the bleeding throne.\n",
    "\n",
    "BISHOP OF ELY:\n",
    "Marry, and will, my lord, to weep in such a one were prettiest;\n",
    "Yet now I was adopted heir\n",
    "Of the world's lamentable day,\n",
    "To watch the next way with his father with his face?\n",
    "\n",
    "ESCALUS:\n",
    "The cause why then we are all resolved more sons.\n",
    "\n",
    "VOLUMNIA:\n",
    "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
    "And love and pale as any will to that word.\n",
    "\n",
    "QUEEN ELIZABETH:\n",
    "But how long have I heard the soul for this world,\n",
    "And show his hands of life be proved to stand.\n",
    "\n",
    "PETRUCHIO:\n",
    "I say he look'd on, if I must be content\n",
    "To stay him from the fatal of our country's bliss.\n",
    "His lordship pluck'd from this sentence then for prey,\n",
    "And then let us twain, being the moon,\n",
    "were she such a case as fills m\n",
    "</pre>\n",
    "\n",
    "虽然有些句子符合语法规则，但是大多数句子没有意义。这个模型尚未学习到单词的含义，但请考虑以下几点：\n",
    "\n",
    "* 此模型是基于字符的。训练开始时，模型不知道如何拼写一个英文单词，甚至不知道单词是文本的一个单位。\n",
    "\n",
    "* 输出文本的结构类似于剧本 -- 文本块通常以讲话者的名字开始；而且与数据集类似，讲话者的名字采用全大写字母。\n",
    "\n",
    "* 如下文所示，此模型由小批次 （batch） 文本训练而成（每批 100 个字符）。即便如此，此模型仍然能生成更长的文本序列，并且结构连贯。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "srXC6pLGLwS6"
   },
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### 导入 TensorFlow 和其他库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version 仅存在于 Colab\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHDoRoc5PKWz"
   },
   "source": [
    "### 下载莎士比亚数据集\n",
    "\n",
    "修改下面一行代码，在你自己的数据上运行此代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "path_to_file = './data/哈利波特全集 1_7部.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### 读取数据\n",
    "\n",
    "首先，看一看文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Length of text: 1676439 characters\n"
    }
   ],
   "source": [
    "# 读取并为 py2 compat 解码\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "\n",
    "# 文本长度是指文本中的字符个数\n",
    "print ('Length of text: {} characters'.format(len(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "哈利波特与魔法石(人文社) 1\n\n\n在他十一岁生日那天，一切都发生了变化，信使猫头鹰带来了一封神秘的信：邀请哈利去一个他——以及所有读到哈利故事的人——会觉得永远难忘的、不可思议的地方——霍格沃茨魔法学校。在魔法学校里，哈利不仅找着了朋友... \n\n作者： \nJ．K．罗琳 /著   \n出版社： \n人民文学出版社 \n\n\n 目录 \n \n        \n \n 主要人物表 \n 第1章 大难不死的男孩 \n 第2章 悄悄消失的玻璃 \n 第3章 猫头鹰传书 \n 第4章 \n"
    }
   ],
   "source": [
    "# 看一看文本中的前 250 个字符\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3944 unique characters\n"
    }
   ],
   "source": [
    "# 文本中的非重复字符\n",
    "vocab = sorted(set(text))\n",
    "print ('{} unique characters'.format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## 处理文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### 向量化文本\n",
    "\n",
    "在训练之前，我们需要将字符串映射到数字表示值。创建两个查找表格：一个将字符映射到数字，另一个将数字映射到字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建从非重复字符到索引的映射\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)\n",
    "\n",
    "text_as_int = np.array([char2idx[c] for c in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tZfqhkYCymwX"
   },
   "source": [
    "现在，每个字符都有一个整数表示值。请注意，我们将字符映射至索引 0 至 `len(unique)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "{\n  '\\n':   0,\n  '\\r':   1,\n  ' ' :   2,\n  '!' :   3,\n  '\"' :   4,\n  \"'\" :   5,\n  '(' :   6,\n  ')' :   7,\n  ',' :   8,\n  '-' :   9,\n  '.' :  10,\n  '/' :  11,\n  '0' :  12,\n  '1' :  13,\n  '2' :  14,\n  '3' :  15,\n  '4' :  16,\n  '5' :  17,\n  '6' :  18,\n  '7' :  19,\n  ...\n}\n"
    }
   ],
   "source": [
    "print('{')\n",
    "for char,_ in zip(char2idx, range(20)):\n",
    "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
    "print('  ...\\n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'哈利波特与魔法石(人文社)' ---- characters mapped to int ---- > [ 674  427 2021 2255  115 3845 2017 2503    6  192 1663 2542    7]\n"
    }
   ],
   "source": [
    "# 显示文本首 13 个字符的整数映射\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bbmsf23Bymwe"
   },
   "source": [
    "### 预测任务"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wssHQ1oGymwe"
   },
   "source": [
    "给定一个字符或者一个字符序列，下一个最可能出现的字符是什么？这就是我们训练模型要执行的任务。输入进模型的是一个字符序列，我们训练这个模型来预测输出 -- 每个时间步（time step）预测下一个字符是什么。\n",
    "\n",
    "由于 RNN 是根据前面看到的元素维持内部状态，那么，给定此时计算出的所有字符，下一个字符是什么？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### 创建训练样本和目标\n",
    "\n",
    "接下来，将文本划分为样本序列。每个输入序列包含文本中的 `seq_length` 个字符。\n",
    "\n",
    "对于每个输入序列，其对应的目标包含相同长度的文本，但是向右顺移一个字符。\n",
    "\n",
    "将文本拆分为长度为 `seq_length+1` 的文本块。例如，假设 `seq_length` 为 4 而且文本为 “Hello”， 那么输入序列将为 “Hell”，目标序列将为 “ello”。\n",
    "\n",
    "为此，首先使用 `tf.data.Dataset.from_tensor_slices` 函数把文本向量转换为字符索引流。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "哈\n利\n波\n特\n与\n"
    }
   ],
   "source": [
    "# 设定每个输入句子长度的最大值\n",
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//seq_length\n",
    "\n",
    "# 创建训练样本 / 目标\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(5):\n",
    "  print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ZSYAcQV8OGP"
   },
   "source": [
    "`batch` 方法使我们能轻松把单个字符转换为所需长度的序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "'哈利波特与魔法石(人文社) 1\\r\\n\\r\\n\\r\\n在他十一岁生日那天，一切都发生了变化，信使猫头鹰带来了一封神秘的信：邀请哈利去一个他——以及所有读到哈利故事的人——会觉得永远难忘的、不可思议的地方——霍格沃茨'\n'魔法学校。在魔法学校里，哈利不仅找着了朋友... \\r\\n\\r\\n作者： \\r\\nJ．K．罗琳 /著   \\r\\n出版社： \\r\\n人民文学出版社 \\r\\n\\r\\n\\r\\n 目录 \\r\\n \\r\\n        \\r\\n \\r\\n 主要人物表 \\r\\n'\n' 第1章 大难不死的男孩 \\r\\n 第2章 悄悄消失的玻璃 \\r\\n 第3章 猫头鹰传书 \\r\\n 第4章 钥匙保管员 \\r\\n 第5章 对角巷 \\r\\n 第6章 从943站台开始的旅程 \\r\\n 第7章 分院帽 \\r\\n 第8章'\n' 魔药课老师 \\r\\n 第9章 午夜决斗 \\r\\n 第10章 万圣节前夜 \\r\\n 第11章 魁地奇比赛 \\r\\n 第12章 厄里斯魔镜 \\r\\n 第13章 尼可勒梅 \\r\\n 第14章 挪威脊背龙—— 诺伯 \\r\\n 第15章 '\n'禁林 \\r\\n 第16章 穿越活板门 \\r\\n 第17章 双面人 \\r\\n \\r\\n \\r\\n哈利.波特，本书主人公，霍格沃茨魔法学校一年级学生\\r\\n佩妮，哈利的姨妈\\r\\n弗农德.思礼，哈利的姨父\\r\\n达力，哈利的表哥，德思礼夫'\n"
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True).repeat()\n",
    "\n",
    "for item in sequences.take(5):\n",
    "  print(repr(''.join(idx2char[item.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UbLcIPBj_mWZ"
   },
   "source": [
    "对于每个序列，使用 `map` 方法先复制再顺移，以创建输入文本和目标文本。`map` 方法可以将一个简单的函数应用到每一个批次 （batch）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hiCopyGZymwi"
   },
   "source": [
    "打印第一批样本的输入与目标值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(100,)\nInput data:  '哈利波特与魔法石(人文社) 1\\r\\n\\r\\n\\r\\n在他十一岁生日那天，一切都发生了变化，信使猫头鹰带来了一封神秘的信：邀请哈利去一个他——以及所有读到哈利故事的人——会觉得永远难忘的、不可思议的地方——霍格沃'\nTarget data: '利波特与魔法石(人文社) 1\\r\\n\\r\\n\\r\\n在他十一岁生日那天，一切都发生了变化，信使猫头鹰带来了一封神秘的信：邀请哈利去一个他——以及所有读到哈利故事的人——会觉得永远难忘的、不可思议的地方——霍格沃茨'\n"
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  input_value = input_example.numpy()\n",
    "  print(input_value.shape)\n",
    "  print('Input data: ', repr(''.join(idx2char[input_value])))\n",
    "  print('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_33OHL3b84i0"
   },
   "source": [
    "这些向量的每个索引均作为一个时间步来处理。作为时间步 0 的输入，模型接收到 “F” 的索引，并尝试预测 “i” 的索引为下一个字符。在下一个时间步，模型执行相同的操作，但是 `RNN` 不仅考虑当前的输入字符，还会考虑上一步的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Step    0\n  input: 674 ('哈')\n  expected output: 427 ('利')\nStep    1\n  input: 427 ('利')\n  expected output: 2021 ('波')\nStep    2\n  input: 2021 ('波')\n  expected output: 2255 ('特')\nStep    3\n  input: 2255 ('特')\n  expected output: 115 ('与')\nStep    4\n  input: 115 ('与')\n  expected output: 3845 ('魔')\n"
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### 创建训练批次\n",
    "\n",
    "前面我们使用 `tf.data` 将文本拆分为可管理的序列。但是在把这些数据输送至模型之前，我们需要将数据重新排列 （shuffle） 并打包为批次。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<RepeatDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 批大小\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 设定缓冲区大小，以重新排列数据集\n",
    "# （TF 数据被设计为可以处理可能是无限的序列，\n",
    "# 所以它不会试图在内存中重新排列整个序列。相反，\n",
    "# 它维持一个缓冲区，在缓冲区重新排列元素。） \n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True).repeat()\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<TakeDataset shapes: ((64, 100), (64, 100)), types: (tf.int32, tf.int32)>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8gPwEjRzf-Z"
   },
   "source": [
    "使用 `tf.keras.Sequential` 定义模型。在这个简单的例子中，我们使用了三个层来定义模型：\n",
    "\n",
    "* `tf.keras.layers.Embedding`：输入层。一个可训练的对照表，它会将每个字符的数字映射到一个 `embedding_dim` 维度的向量。 \n",
    "* `tf.keras.layers.GRU`：一种 RNN 类型，其大小由 `units=rnn_units` 指定（这里你也可以使用一个 LSTM 层）。\n",
    "* `tf.keras.layers.Dense`：输出层，带有 `vocab_size` 个输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 词集的长度\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# 嵌入的维度\n",
    "embedding_dim = 256\n",
    "\n",
    "# RNN 的单元数量\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    # stateful：继承状态\n",
    "    tf.keras.layers.GRU(rnn_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### 配置检查点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C6XBUUavgF56"
   },
   "source": [
    "使用 `tf.keras.callbacks.ModelCheckpoint` 来确保训练过程中保存检查点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查点保存至的目录\n",
    "checkpoint_dir = './data/robot_model'\n",
    "\n",
    "# 检查点的文件名\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x202d049f348>"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载权重\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-42-f63e51847bc3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-42-f63e51847bc3>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    对于每个字符，模型会查找嵌入，把嵌入当作输入运行 GRU 一个时间步，并用密集层生成逻辑回归 （logits），预测下一个字符的对数可能性。\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "对于每个字符，模型会查找嵌入，把嵌入当作输入运行 GRU 一个时间步，并用密集层生成逻辑回归 （logits），预测下一个字符的对数可能性。\n",
    "![数据在模型中传输的示意图](https://github.com/littlebeanbean7/docs/blob/master/site/en/tutorials/text/images/text_generation_training.png?raw=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ubPo0_9Prjb"
   },
   "source": [
    "## 试试这个模型\n",
    "\n",
    "现在运行这个模型，看看它是否按预期运行。\n",
    "\n",
    "首先检查输出的形状："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(64, 100) (64, 100) # (batch_size, sequence_length, vocab_size)\n(64, 100, 3944) # (batch_size, sequence_length, vocab_size)\n"
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  print(input_example_batch.shape, target_example_batch.shape, \"# (batch_size, sequence_length)\")\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q6NzLBi4VM4o"
   },
   "source": [
    "在上面的例子中，输入的序列长度为 `100`， 但是这个模型可以在任何长度的输入上运行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (64, None, 256)           1009664   \n_________________________________________________________________\ngru (GRU)                    (64, None, 1024)          3938304   \n_________________________________________________________________\ndense (Dense)                (64, None, 3944)          4042600   \n=================================================================\nTotal params: 8,990,568\nTrainable params: 8,990,568\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwv0gEkURfx1"
   },
   "source": [
    "为了获得模型的实际预测，我们需要从输出分布中抽样，以获得实际的字符索引。这个分布是根据对字符集的逻辑回归定义的。\n",
    "\n",
    "请注意：从这个分布中 _抽样_ 很重要，因为取分布的 _最大值自变量点集（argmax）_ 很容易使模型卡在循环中。\n",
    "\n",
    "试试这个批次中的第一个样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QM1Vbxs_URw5"
   },
   "source": [
    "这使我们得到每个时间步预测的下一个字符的索引。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1072, 1969, 1925, 3845,  741,  288,  674,  596,  846,  601, 1083,\n         98, 1465, 1677, 2237,  901,   98, 3829, 1178, 2171, 1993, 2211,\n        114,  180, 1042,  149,   98, 1022, 3914,  246, 1715, 2452, 3885,\n       3888,  922,  553, 2418, 3181, 2994, 1665, 1083, 2340, 2418, 3238,\n       1160, 3926,   87,   14,   13,    9,    9,  674,  819, 2187, 2001,\n        922, 2418,   88, 1279,  144,  719, 3358, 2187,  412,  922, 2418,\n         26,  674,  674,    1,    0,    1,    0,    2,    2,  674,  336,\n       1253, 1677, 2237,  901, 1719, 1248, 1759, 2343, 2607,   98, 2452,\n        112, 1715,  214,  232, 1079, 3453,  130, 1679, 3760,   98,    2,\n          1], dtype=int64)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfLtsP3mUhCG"
   },
   "source": [
    "解码它们，以查看此未经训练的模型预测的文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Input: \n '比较喜欢你们叫我敏西的尼古拉斯爵士。”幽灵显得有些局促不安，但是淡茶色头发的西莫斐尼甘插话说：-75 -“差点没头?你怎么会差点没头呢?” \\r\\n\\r\\n  尼古拉斯爵士显得很生气，看来他不想谈这个话题。 '\n\nNext Char Predictions: \n '尔气欢魔喜俩哈吗坐吧尼。拉斯爵士。高幽灵沉然不些害乎。安，但是看黄黑头发的西莫斐尼甘的话带：“21--哈因点没头的”怎么啦跑点分头的?哈哈\\r\\n\\r\\n  哈克德斯爵士显得有生立。看上是们会就这个方题。 \\r'\n"
    }
   ],
   "source": [
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YCbHQHiaa4Ic"
   },
   "source": [
    "此时，这个问题可以被视为一个标准的分类问题：给定先前的 RNN 状态和这一时间步的输入，预测下一个字符的类别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "trpqTWyvk0nr"
   },
   "source": [
    "### 添加优化器和损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UAjbjY03eiQ4"
   },
   "source": [
    "标准的 `tf.keras.losses.sparse_categorical_crossentropy` 损失函数在这里适用，因为它被应用于预测的最后一个维度。\n",
    "\n",
    "因为我们的模型返回逻辑回归，所以我们需要设定命令行参数 `from_logits`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Prediction shape:  (64, 100, 3944)  # (batch_size, sequence_length, vocab_size)\nscalar_loss:       1.2150961\n"
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jeOXriLcymww"
   },
   "source": [
    "使用 `tf.keras.Model.compile` 方法配置训练步骤。我们将使用 `tf.keras.optimizers.Adam` 并采用默认参数，以及损失函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### 执行训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IxdOA-rgyGvs"
   },
   "source": [
    "为保持训练时间合理，使用 10 个周期来训练模型。在 Colab 中，将运行时设置为 GPU 以加速训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Train for 300 steps\nEpoch 1/30\n300/300 [==============================] - 70s 235ms/step - loss: 1.5313\nEpoch 2/30\n300/300 [==============================] - 67s 222ms/step - loss: 1.5399\nEpoch 3/30\n300/300 [==============================] - 68s 227ms/step - loss: 1.5449\nEpoch 4/30\n300/300 [==============================] - 69s 229ms/step - loss: 1.5393\nEpoch 5/30\n300/300 [==============================] - 78s 261ms/step - loss: 1.5366\nEpoch 6/30\n300/300 [==============================] - 70s 233ms/step - loss: 1.5240\nEpoch 7/30\n300/300 [==============================] - 69s 229ms/step - loss: 1.5118\nEpoch 8/30\n300/300 [==============================] - 67s 223ms/step - loss: 1.5001\nEpoch 9/30\n300/300 [==============================] - 66s 219ms/step - loss: 1.4874\nEpoch 10/30\n300/300 [==============================] - 69s 230ms/step - loss: 1.4752\nEpoch 11/30\n300/300 [==============================] - 72s 240ms/step - loss: 1.4649\nEpoch 12/30\n300/300 [==============================] - 70s 232ms/step - loss: 1.4525\nEpoch 13/30\n300/300 [==============================] - 68s 225ms/step - loss: 1.4431\nEpoch 14/30\n300/300 [==============================] - 69s 229ms/step - loss: 1.4336\nEpoch 15/30\n300/300 [==============================] - 66s 221ms/step - loss: 1.4230\nEpoch 16/30\n300/300 [==============================] - 67s 224ms/step - loss: 1.4119\nEpoch 17/30\n300/300 [==============================] - 68s 225ms/step - loss: 1.4038\nEpoch 18/30\n300/300 [==============================] - 67s 225ms/step - loss: 1.3980\nEpoch 19/30\n300/300 [==============================] - 67s 223ms/step - loss: 1.3911\nEpoch 20/30\n300/300 [==============================] - 65s 218ms/step - loss: 1.3842\nEpoch 21/30\n300/300 [==============================] - 65s 218ms/step - loss: 1.3800\nEpoch 22/30\n300/300 [==============================] - 66s 219ms/step - loss: 1.3699\nEpoch 23/30\n300/300 [==============================] - 65s 217ms/step - loss: 1.3666\nEpoch 24/30\n300/300 [==============================] - 65s 216ms/step - loss: 1.3621\nEpoch 25/30\n300/300 [==============================] - 65s 217ms/step - loss: 1.3574\nEpoch 26/30\n300/300 [==============================] - 65s 215ms/step - loss: 1.3512\nEpoch 27/30\n300/300 [==============================] - 65s 217ms/step - loss: 1.3474\nEpoch 28/30\n300/300 [==============================] - 66s 219ms/step - loss: 1.3454\nEpoch 29/30\n300/300 [==============================] - 68s 226ms/step - loss: 1.3457\nEpoch 30/30\n300/300 [==============================] - 66s 222ms/step - loss: 1.3374\n"
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback], steps_per_epoch=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## 生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JIPcXllKjkdr"
   },
   "source": [
    "### 恢复最新的检查点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LyeYRiuVjodY"
   },
   "source": [
    "为保持此次预测步骤简单，将批大小设定为 1。\n",
    "\n",
    "由于 RNN 状态从时间步传递到时间步的方式，模型建立好之后只接受固定的批大小。\n",
    "\n",
    "若要使用不同的 `batch_size` 来运行模型，我们需要重建模型并从检查点中恢复权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'./data/robot_model\\\\ckpt_9'"
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Model: \"sequential_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_3 (Embedding)      (1, None, 256)            1009664   \n_________________________________________________________________\ngru_3 (GRU)                  (1, None, 1024)           3938304   \n_________________________________________________________________\ndense_3 (Dense)              (1, None, 3944)           4042600   \n=================================================================\nTotal params: 8,990,568\nTrainable params: 8,990,568\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### 预测循环\n",
    "\n",
    "下面的代码块生成文本：\n",
    "\n",
    "* 首先设置起始字符串，初始化 RNN 状态并设置要生成的字符个数。\n",
    "\n",
    "* 用起始字符串和 RNN 状态，获取下一个字符的预测分布。\n",
    "\n",
    "* 然后，用分类分布计算预测字符的索引。把这个预测字符当作模型的下一个输入。\n",
    "\n",
    "* 模型返回的 RNN 状态被输送回模型。现在，模型有更多上下文可以学习，而非只有一个字符。在预测出下一个字符后，更改过的 RNN 状态被再次输送回模型。模型就是这样，通过不断从前面预测的字符获得更多上下文，进行学习。\n",
    "\n",
    "![为生成文本，模型的输出被输送回模型作为输入](https://github.com/littlebeanbean7/docs/blob/master/site/en/tutorials/text/images/text_generation_sampling.png?raw=1)\n",
    "\n",
    "查看生成的文本，你会发现这个模型知道什么时候使用大写字母，什么时候分段，而且模仿出了莎士比亚式的词汇。由于训练的周期小，模型尚未学会生成连贯的句子。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # 评估步骤（用学习过的模型生成文本）\n",
    "\n",
    "  # 要生成的字符个数\n",
    "  num_generate = 1000\n",
    "\n",
    "  # 将起始字符串转换为数字（向量化）\n",
    "  input_eval = [char2idx[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # 空字符串用于存储结果\n",
    "  text_generated = []\n",
    "\n",
    "  # 低温度会生成更可预测的文本\n",
    "  # 较高温度会生成更令人惊讶的文本\n",
    "  # 可以通过试验以找到最好的设定\n",
    "  temperature = 1.0\n",
    "\n",
    "  # 这里批大小为 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # 删除批次的维度\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # 用分类分布预测模型返回的字符\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "      # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "一天浑然不同了。 \n\n哈利看见乔治瞟了金丽?韦斯莱孪生兄弟那袋边得格许多，\n\n“我好像不能算样的！”赫敏帮忙忙问……反正，我都会愿意的。” \n\n  罗恩狂叫起来。 \n\n  “黑魔头?我没在滑到帮帮上面，所以有那套上石雕刻，就在它们之间为什么不来一趟医院去了?” “没关系，”乔治急碌地说，“ 不祥。” “为什么呢?”德拉科马尔福喜颜大的?”“赫敏，你居然在麻瓜居然偷偷地跟前它也不要这么。”哈利板着脸。然后她向来伸活：快端正从阿兹卡班逃跑的那件事小裂缝过后，他不能拦腰布里克发出挑衅的神情。\n\n“是啊，住的露劳福都是此学校的。首相和查理的关系也有一段时间了。\n\n“女士们家里有许多懂事我们需要高大酒吧。”哈利皱巴巴，就在他前面。\n\n“我对你父亲感觉一个人待着对付，所以，让你一个人也忙忙这样。”\n\n    “太好了，太好了。”今天的晨电话。他对视着水草的那把玻璃投来整脖颈段的小木卫，大声说道，但立即开始头绕着一些同学的声音。 \n\n  “幻是—— 对不起。罗恩，我们不用窥镜，”其他的人嘴巴和面对方面瞪大眼昏门，“我想你明白做什么不是有的事情，实际上装伯魅时能做出越来的文字。是万斯的倒是征兆，而一言本帮助了，他那金喜色眼睛看着对方的盘子都没有)。那么恶狠狠地说，又一次抽了出去，直到肋部缩在脖子上，好像就是神秘人刀。哈利觉得要是哪些又吸又吱吱呀……唉，哈利以为自己想象不出这次更好考虑未成的防御摄魂怪……\n\n伏地魔就再也没有被选效，丽塔?格斯普劳奇的同类\n\n进来吧。然而，听同了一堆真深的话，似乎对他们无损失望。\n\n“没有，”当哈利在他们看见星期三更多的时候，顶着一个小路上，“伯莎利娜先生？？”\n\n    “行——总是这样。”哈利说，紧挨着脑袋裹着分量扬起了眉毛。\n\n    “噢，”韦斯莱夫人说着，自己又回到伏地魔脑门刚才梦见了。似乎只感到韦斯莱先生的那根被墨水落到了柜子上，里面是有的，光线昏暗；哈利见过屋两人，拼命克制着根本没有多得多惊讶，这样他们早就忍不住可笑了。洛夫斯卢多询问哈利博德即也嘴里那张丑陋的意料中，哈利仍然在晚上俯身被打量的目光看出了他的脸，看到那女人站在地板上以来仿佛他看见的那样东西——“紧张把信打开！”穆迪说，语气里大得不得直直往后退了一步。麦克拉根就四跳了地，上楼吃早饭时，他的脸色蔚蓝红得像忙长的火花。哈利听见弗\n"
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"一天\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AM2Uma_-yVIq"
   },
   "source": [
    "若想改进结果，最简单的方式是延长训练时间 （试试 `EPOCHS=30`）。\n",
    "\n",
    "你还可以试验使用不同的起始字符串，或者尝试增加另一个 RNN 层以提高模型的准确率，亦或调整温度参数以生成更多或者更少的随机预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y4QwTjAM6A2O"
   },
   "source": [
    "## 高级：自定义训练\n",
    "\n",
    "上面的训练步骤简单，但是能控制的地方不多。\n",
    "\n",
    "至此，你已经知道如何手动运行模型。现在，让我们打开训练循环，并自己实现它。这是一些任务的起点，例如实现 _课程学习_ 以帮助稳定模型的开环输出。\n",
    "\n",
    "你将使用 `tf.GradientTape` 跟踪梯度。关于此方法的更多信息请参阅 [eager execution 指南](https://www.tensorflow.org/guide/eager)。\n",
    "\n",
    "步骤如下：\n",
    "\n",
    "* 首先，初始化 RNN 状态，使用 `tf.keras.Model.reset_states` 方法。\n",
    "\n",
    "* 然后，迭代数据集（逐批次）并计算每次迭代对应的 *预测*。\n",
    "\n",
    "* 打开一个 `tf.GradientTape` 并计算该上下文时的预测和损失。\n",
    "\n",
    "* 使用 `tf.GradientTape.grads` 方法，计算当前模型变量情况下的损失梯度。\n",
    "\n",
    "* 最后，使用优化器的 `tf.train.Optimizer.apply_gradients` 方法向下迈出一步。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, target):\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(inp)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.keras.losses.sparse_categorical_crossentropy(\n",
    "            target, predictions, from_logits=True))\n",
    "  grads = tape.gradient(loss, model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Epoch 1 Batch 0 Loss 3.1333320140838623\nEpoch 1 Batch 100 Loss 2.9367525577545166\nEpoch 1 Batch 200 Loss 2.9673290252685547\nEpoch 1 Loss 2.8993\nTime taken for 1 epoch 54.37251687049866 sec\n\nEpoch 2 Batch 0 Loss 2.7713282108306885\nEpoch 2 Batch 100 Loss 2.790792226791382\nEpoch 2 Batch 200 Loss 2.7311131954193115\nEpoch 2 Loss 2.7933\nTime taken for 1 epoch 54.43235802650452 sec\n\nEpoch 3 Batch 0 Loss 2.5789449214935303\nEpoch 3 Batch 100 Loss 2.6261913776397705\nEpoch 3 Batch 200 Loss 2.6724724769592285\nEpoch 3 Loss 2.5198\nTime taken for 1 epoch 53.617740631103516 sec\n\nEpoch 4 Batch 0 Loss 2.482571601867676\nEpoch 4 Batch 100 Loss 2.6415796279907227\nEpoch 4 Batch 200 Loss 2.2979094982147217\nEpoch 4 Loss 2.4346\nTime taken for 1 epoch 53.67239165306091 sec\n\nEpoch 5 Batch 0 Loss 2.4212262630462646\nEpoch 5 Batch 100 Loss 2.428463935852051\nEpoch 5 Batch 200 Loss 2.4513132572174072\nEpoch 5 Loss 2.2703\nTime taken for 1 epoch 54.26480674743652 sec\n\nEpoch 6 Batch 0 Loss 2.145399570465088\nEpoch 6 Batch 100 Loss 2.2973101139068604\nEpoch 6 Batch 200 Loss 2.1752421855926514\nEpoch 6 Loss 2.2700\nTime taken for 1 epoch 53.92671084403992 sec\n\nEpoch 7 Batch 0 Loss 1.912786602973938\nEpoch 7 Batch 100 Loss 2.3700451850891113\nEpoch 7 Batch 200 Loss 2.148899555206299\nEpoch 7 Loss 2.1277\nTime taken for 1 epoch 53.700316429138184 sec\n\nEpoch 8 Batch 0 Loss 1.884372591972351\nEpoch 8 Batch 100 Loss 2.1484153270721436\nEpoch 8 Batch 200 Loss 2.057189702987671\nEpoch 8 Loss 2.0321\nTime taken for 1 epoch 53.64546346664429 sec\n\nEpoch 9 Batch 0 Loss 1.7241238355636597\nEpoch 9 Batch 100 Loss 2.061195135116577\nEpoch 9 Batch 200 Loss 1.8444920778274536\nEpoch 9 Loss 2.0030\nTime taken for 1 epoch 53.68136787414551 sec\n\nEpoch 10 Batch 0 Loss 1.7339633703231812\nEpoch 10 Batch 100 Loss 2.0370917320251465\nEpoch 10 Batch 200 Loss 1.8728190660476685\nEpoch 10 Loss 1.8679\nTime taken for 1 epoch 54.36254405975342 sec\n\n"
    }
   ],
   "source": [
    "# 训练步骤\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  # 在每个训练周期开始时，初始化隐藏状态\n",
    "  # 隐藏状态最初为 None\n",
    "  hidden = model.reset_states()\n",
    "\n",
    "  for (batch_n, (inp, target)) in enumerate(dataset.take(300)):\n",
    "    loss = train_step(inp, target)\n",
    "\n",
    "    if batch_n % 100 == 0:\n",
    "      template = 'Epoch {} Batch {} Loss {}'\n",
    "      print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "  # 每 5 个训练周期，保存（检查点）1 次模型\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "  print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "  print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "text_generation.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}